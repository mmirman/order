\section{The Calculus of Constructions}

The type system for Caledon is based on the ``Calculus of Constructions'' as defined by Coquand \citep{coquand1986calculus}.
Since Caledon might extend the ``Calculus of Constructions,'' it is important to view it as a pure type system. 
As Barendgregt points out, the common type theories can be recast as pure type systems
by choice of axioms. 

Roorda \citep{roorda2001pure} gave an implementation of a functional programming language with 
pure type system and demonstrated its utility.

This is the pure type system where.

\begin{definition}
\textbf{(PTS for $CC$)}

\begin{align}
A &= \{ *, \Box \}
\\
S &= \{ (* : \Box) \}
\\
R &= \{ (*,*,*),(*,\Box,\Box),(\Box,\Box,\Box),(\Box,*,*)\}
\end{align}  

\label{coc:types}
\end{definition}

In this system, terms can depend on terms and types, 
and types can depend on types and terms.  
This pure type system has the well known strong normalization property, implying the termination of 
all lambda terms typeable by $CC$ \citep{Geuvers94ashort} \citep{geuvers1991modular}.
It is necessary to be careful with the types of equalities allowed in the conversion rule, 
since if there are more allowed equalities, then certain proofs
become significantly more complex.

\begin{definition}
$\Gamma \vdash_{CC} P : T : K$ means $\Gamma \vdash_{CC} P : T$ and $\Gamma \vdash_{CC} T : K$
\end{definition}


\subsection{Consistency of the Calculus of Constructions}

\begin{definition}
$ \m{Term}_{CC}  = \{ M : \exists T,\Gamma . \Gamma \vdash_{cc} M : T \}$
\end{definition}

\begin{theorem}
\textbf{(Strong Normalization)} $\forall M \in \m{Term}_{CC}. SN(M)$
\label{cc:cons}
\end{theorem}

The easiest to digest proof is also due to Geuvers \citep{Geuvers94ashort} 
but other proofs have also been proposed.  Geuvers' proof has the convenient
property that it does not depend too much on the definition of the set $SN$ 
(strongly normalizing). The only properties required are that $S \subseteq SN$ 
where $S$ is the set of sorts in the system. In the case of $CC$ $\Box,* \in SN$.
It also requires that $\Pi x : A . B \in SN$, and $\lambda x : A . B \in SN$ 
for any $A,B$.  For those familiar with Geuvers' proof, 
it is important to note that the proof requires that saturated subsets of $SN$ are closed under
arbitrary intersection and function space generation.

This proof is restricted to normalization in the calculus where only $\beta$ reduction 
is considered and not $\eta$ conversion.  The proof for the calculus with full reduction properties is
in Guevers' thesis \citep{geuvers1993logics}.  While normalization of terms in Caledon without considering proof search does
not involve $\eta$ conversion, unification could potentially involve $\eta$ expansion and it is helpful to maintain
the consistency of the ``Calculus of Constructions'' even when $\eta$ reduction is considered.  
It is also important to note that in the Curry-style calculus where types are omitted from lambda abstractions, 
the Church-Rosser theorem under $\eta$ conversion appears essentially for free\citep{miquel2001implicit}. Strong normalization 
follows, and by replacement of types, strong normalization follows for the Curry-style calculus.

\subsection{Impredicativity in the Calculus of Constructions}

It is also important to note that while the term language of the calculus of constructions is strongly normalizing, 
the predicates in the calculus of constructions are impredicative, meaning that small types (meaning propositions) can be generalized over all small types.
This adds yet another layer of computation into the Caledon language which might not terminate.  
Furthermore, the predicate language is insufficient to prove properties of larger types.  For example, 
we'd be unable to encode or prove many notions from category theory given that they would be required to apply to the 
category of predicates.  
Luo \citep{luo1989ecc} solves this by introducing universes into the ``Extended Calculus of Constructions''.  
As defined by Luo, the Extended Calculus of Constructions is no longer a pure type system.

\citet{harper1991type} provided an analysis of a few pure type systems with universes.
Agda, Idris, Coq,
and Plastic \citep{callaghan2001implementation} all implement this technique.
Idris uses constraint satisfaction to allow for implicit universes
and thus does not allow the universes to become an annoyance in code. 
While not discussed in detail in the thesis, the Caledon implementation similarly produces constraints on implicit 
universes during unification and reduces the cyclic universe checking problem to dynamic transitive closure.

\subsection{Theorems in Caledon}

It is important to note that in the programming language Caledon, programs might not terminate.  
The search language itself is not consistent, and is not a theorem verification language like LF.  
Rather, it is language for writing theorem-proving programs.  

\begin{definition}
In the Caledon language, $\m{prop} = *$ and $\m{type} = \Box$
\end{definition}

It is important to note that in Caledon code, $\m{type}$ will never appear.  This is because $\m{type} : T$ for any $T$
is not provable in the Calculus of Constructions, and every term that appears in the language must have a provable type.

\begin{definition}
If $\Gamma \vdash_{CICC} P : T : \m{prop}$ in the Caledon language, then $T$ is a theorem, and $P$ is a proof.
\end{definition}

Terms can be considered this way since the Calculus of Constructions is consistent.
No unbounded computation is necessary to normalize $P$.
Specifically, proof search is not involved in the normalization procedure.

When $CC$ was first developed, theorems were proven and generated by explicitly defining
constructors and destructors for records and sum types.  Later, the inductive Calculus of Constructions was developed 
\citep{coquand1990inductively} which more accurately forms the basis of the Coq programming language.  These types of inductive
constructions have been omitted from Caledon.  Instead, predicates are specified by assuming axioms relating them together.
This omits the confusion that would be generated from having both inductively defined data and predicates in the system, as 
dependently typed logic programming treats predicates as both data and code.

Unfortunately, simple $CC$ does not make for an expressive or useful theorem proving language.  
This can significantly limit the utility of theorem searching techniques.  
In order to make it more useful, a predicative hierarchy of universes 
was appended to the language which would allow for inclusion of more meta-mathematics techniques.
As it turns out, this universe hierarchy is can be essential to meta-programming in Caledon.  
While most of the rest of thesis assumes a simple impredicative universe, I explain the 
universe construction here, and allow the reader to extrapolate for the remainder of the thesis.

The original calculus of constructions with a universe hierarchy included an impredicative type.  

\begin{definition}
\textbf{(PTS for $CC_\omega$)}

\[
A = \{ \m{prop} \} \cup \{ \m{type}_i | i \in \mathbb{N} \}
\]

\[
S =   \{ (\m{prop} : \m{type}_0 ) \}
 \cup \{ (\m{type}_i : \m{type}_{i+1} ) | i \in \mathbb{N} \}
\]

\[ 
R = \{ (\m{type}_j,\m{type}_i,\m{type}_k) | j \leq k \wedge i \leq k \}
\cup \{ (\m{prop},t,t) | t \in A \}
\cup \{ (t,\m{prop},t) | t \in A \}
\]

\label{coc:utypes}
\end{definition}

While type checking with the addition of the impredicative universe is theoretically equivalently as difficult 
as without it, in practice it turns out to not be very useful for proof or program writing, and 
languages like CoQ, Agda and Idris now omit it for the system with only predicative universes.

The addition of a universe heirachy into the Caledon Language is possible without too much added difficulty. 
Experimentation shows that the omitting the predicative $\m{prop}$ allows for a simpler implementation of type inference.
In this case, typechecking and unification are performed with the usually inconsistent assumption that
$\m{type} : \m{type}$, and a cycle checker is later applied to ensure that there is no instance of 
$\m{type}_i : \m{type}_i$ necessary.  

In the case where an impredicative universe is included, the extra axiom $\m{prop} : \m{type}$ would
need to be included, which would mean searching among two possibilities, rather than a single possibility 
when attempting to find a type to replace a proof hole with. 
The added nondeterminism tends to cause an explosion in the time complexity of type inference.
